{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-02T08:49:51.609430Z",
     "iopub.status.busy": "2024-09-02T08:49:51.608977Z",
     "iopub.status.idle": "2024-09-02T08:49:57.606714Z",
     "shell.execute_reply": "2024-09-02T08:49:57.605773Z",
     "shell.execute_reply.started": "2024-09-02T08:49:51.609339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import expand_dims\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, Lambda\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T08:49:57.608358Z",
     "iopub.status.busy": "2024-09-02T08:49:57.607958Z",
     "iopub.status.idle": "2024-09-02T08:50:07.701271Z",
     "shell.execute_reply": "2024-09-02T08:50:07.700260Z",
     "shell.execute_reply.started": "2024-09-02T08:49:57.608322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install imutils\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize timing variable\n",
    "cumulative_time = 0.0\n",
    "\n",
    "def create_clients(image_list, label_list, num_clients=100, initial='clients'):\n",
    "    \"\"\"\n",
    "    Create client data shards for federated learning\n",
    "    \n",
    "    Returns: \n",
    "        A dictionary with keys as clients' names and values as data shards\n",
    "        (tuples of images and label lists)\n",
    "    \n",
    "    Args: \n",
    "        image_list: List of numpy arrays of training images\n",
    "        label_list: List of binarized labels for each image\n",
    "        num_clients: Number of federated members (clients)\n",
    "        initial: The clients' name prefix, e.g., 'client_1'\n",
    "    \"\"\"\n",
    "    # Create a list of client names\n",
    "    client_names = [f'{initial}_{i+1}' for i in range(num_clients)]\n",
    "\n",
    "    # Sort data for non-IID distribution\n",
    "    max_y = np.argmax(label_list, axis=-1)\n",
    "    sorted_zip = sorted(zip(max_y, label_list, image_list), key=lambda x: x[0])\n",
    "    data = [(x, y) for _, y, x in sorted_zip]\n",
    "\n",
    "    # Shard data and place at each client\n",
    "    size = len(data) // num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size * num_clients, size)]\n",
    "\n",
    "    # Number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
    "\n",
    "\n",
    "def batch_data(data_shard, batch_size=32):\n",
    "    \"\"\"\n",
    "    Batches data for training with proper shuffling\n",
    "    \n",
    "    Args:\n",
    "        data_shard: List of (image, label) tuples\n",
    "        batch_size: Size of each batch\n",
    "    \n",
    "    Returns:\n",
    "        TensorFlow dataset batched and shuffled\n",
    "    \"\"\"\n",
    "    # Separate images and labels\n",
    "    data_X = np.array([data[0] for data in data_shard])\n",
    "    data_Y = np.array([data[1] for data in data_shard])\n",
    "    \n",
    "    # Create TF dataset with shuffling and batching\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data_X, data_Y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(data_shard))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def weight_scalling_factor(clients_batched, client_name):\n",
    "    \"\"\"\n",
    "    Calculate scaling factor for a client based on dataset size\n",
    "    \n",
    "    Formula: N_k / N (client's data size / total data size)\n",
    "    \n",
    "    Args:\n",
    "        clients_batched: Dictionary of client datasets\n",
    "        client_name: Name of the current client\n",
    "    \n",
    "    Returns:\n",
    "        Scaling factor for the client\n",
    "    \"\"\"\n",
    "    # Get the batch data\n",
    "    client_dataset = clients_batched[client_name]\n",
    "    \n",
    "    # Count number of samples for this client\n",
    "    client_len = sum([len(batch[0]) for batch in client_dataset])\n",
    "    \n",
    "    # Count total number of samples across all participating clients\n",
    "    total_len = sum([sum([len(batch[0]) for batch in client_data]) \n",
    "                     for client_data in clients_batched.values()])\n",
    "    \n",
    "    return client_len / total_len\n",
    "\n",
    "\n",
    "def scale_model_weights(weights, scalar):\n",
    "    \"\"\"\n",
    "    Scale model weights by a scalar value\n",
    "    \n",
    "    Args:\n",
    "        weights: List of numpy arrays (model weights)\n",
    "        scalar: Scaling factor\n",
    "    \n",
    "    Returns:\n",
    "        Scaled weights\n",
    "    \"\"\"\n",
    "    return [w * scalar for w in weights]\n",
    "\n",
    "\n",
    "def apply_threshold_normalization(weight_updates, threshold_percentile=95):\n",
    "    \"\"\"\n",
    "    Apply threshold-based normalization to prevent excessive updates\n",
    "    \n",
    "    This constrains large weight updates that could destabilize training,\n",
    "    particularly useful when dealing with noisy or anomalous client data.\n",
    "    \n",
    "    Args:\n",
    "        weight_updates: List of weight arrays\n",
    "        threshold_percentile: Percentile to use as clipping threshold (default: 95)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized weight updates\n",
    "    \"\"\"\n",
    "    normalized_updates = []\n",
    "    \n",
    "    for weight_layer in weight_updates:\n",
    "        # Calculate threshold based on percentile of absolute values\n",
    "        abs_values = np.abs(weight_layer)\n",
    "        threshold = np.percentile(abs_values, threshold_percentile)\n",
    "        \n",
    "        # Clip values exceeding threshold\n",
    "        clipped_weights = np.clip(weight_layer, -threshold, threshold)\n",
    "        normalized_updates.append(clipped_weights)\n",
    "    \n",
    "    return normalized_updates\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    \"\"\"\n",
    "    Sum scaled weights from multiple clients with threshold normalization\n",
    "    \n",
    "    Implements: W_t = Σ(N_k/N * W_k^t) with threshold normalization\n",
    "    \n",
    "    Args:\n",
    "        scaled_weight_list: List of scaled weight lists from clients\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated weights\n",
    "    \"\"\"\n",
    "    # Initialize with zeros matching the shape of first client's weights\n",
    "    avg_weights = [np.zeros_like(w) for w in scaled_weight_list[0]]\n",
    "    \n",
    "    # Sum all scaled weights\n",
    "    for scaled_weights in scaled_weight_list:\n",
    "        for i, weight_layer in enumerate(scaled_weights):\n",
    "            avg_weights[i] += weight_layer\n",
    "    \n",
    "    # Apply threshold normalization to prevent extreme updates\n",
    "    avg_weights = apply_threshold_normalization(avg_weights, threshold_percentile=95)\n",
    "    \n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "def compute_weight_divergence(old_weights, new_weights):\n",
    "    \"\"\"\n",
    "    Compute L2 divergence between old and new weights\n",
    "    Useful for monitoring model stability\n",
    "    \n",
    "    Args:\n",
    "        old_weights: Previous model weights\n",
    "        new_weights: New model weights\n",
    "    \n",
    "    Returns:\n",
    "        L2 norm of the difference\n",
    "    \"\"\"\n",
    "    divergence = 0.0\n",
    "    for old_w, new_w in zip(old_weights, new_weights):\n",
    "        divergence += np.sum((old_w - new_w) ** 2)\n",
    "    return np.sqrt(divergence)\n",
    "\n",
    "\n",
    "print(\"✓ Federated Learning helper functions loaded successfully\")\n",
    "print(\"  - create_clients: Creates client data shards for federated learning\")\n",
    "print(\"  - batch_data: Creates batched TF datasets with shuffling\")\n",
    "print(\"  - weight_scalling_factor: Computes client contribution weight (N_k/N)\")\n",
    "print(\"  - scale_model_weights: Scales weights by scalar factor\")\n",
    "print(\"  - apply_threshold_normalization: Clips extreme weight updates\")\n",
    "print(\"  - sum_scaled_weights: Aggregates scaled weights with normalization\")\n",
    "print(\"  - compute_weight_divergence: Monitors model update stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T08:50:07.709499Z",
     "iopub.status.busy": "2024-09-02T08:50:07.709246Z",
     "iopub.status.idle": "2024-09-02T08:50:07.732434Z",
     "shell.execute_reply": "2024-09-02T08:50:07.731593Z",
     "shell.execute_reply.started": "2024-09-02T08:50:07.709475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(X_test, Y_test, model, comm_round):\n",
    "    global cumulative_time  # Ensure we're modifying the global cumulative time variable\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    loss = float(cce(Y_test, preds).numpy())\n",
    "    y_true = np.argmax(Y_test, axis=1)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    cumulative_time += elapsed_time\n",
    "\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {:.4f} | cumulative_time: {:.2f} seconds'.format(\n",
    "        comm_round, acc, loss, cumulative_time\n",
    "    ))\n",
    "\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T08:50:07.734112Z",
     "iopub.status.busy": "2024-09-02T08:50:07.733807Z",
     "iopub.status.idle": "2024-09-02T08:50:07.756810Z",
     "shell.execute_reply": "2024-09-02T08:50:07.755891Z",
     "shell.execute_reply.started": "2024-09-02T08:50:07.734082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED MACNN ARCHITECTURE WITH BETTER INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "\n",
    "def attention_module(inputs):\n",
    "    \"\"\"Squeeze-and-Excitation block with improved design\"\"\"\n",
    "    filters = inputs.shape[-1]\n",
    "    reduction = max(filters // 16, 1)\n",
    "\n",
    "    se = layers.GlobalAveragePooling2D()(inputs)\n",
    "    se = layers.Dense(units=reduction, activation='relu', \n",
    "                      kernel_initializer='he_normal')(se)\n",
    "    se = layers.Dense(units=filters, activation='sigmoid',\n",
    "                      kernel_initializer='he_normal')(se)\n",
    "    se = layers.Reshape((1, 1, filters))(se)\n",
    "\n",
    "    return layers.Multiply()([inputs, se])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    \"\"\"Spatial attention mechanism\"\"\"\n",
    "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=3, keepdims=True))(input_feature)\n",
    "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=3, keepdims=True))(input_feature)\n",
    "    concat = layers.Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\n",
    "    attention_map = layers.Conv2D(filters=1, kernel_size=7, strides=1, padding='same', \n",
    "                                   activation='sigmoid', kernel_initializer='he_normal')(concat)\n",
    "    return layers.Multiply()([input_feature, attention_map])\n",
    "\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \"\"\"Channel attention mechanism with shared weights\"\"\"\n",
    "    channel = input_feature.shape[-1]\n",
    "    reduction = max(channel // ratio, 1)\n",
    "    shared_dense_one = layers.Dense(reduction, activation='relu', \n",
    "                                     kernel_initializer='he_normal')\n",
    "    shared_dense_two = layers.Dense(channel, activation='sigmoid',\n",
    "                                     kernel_initializer='he_normal')\n",
    "\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    avg_out = shared_dense_two(shared_dense_one(avg_pool))\n",
    "\n",
    "    max_pool = layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_out = shared_dense_two(shared_dense_one(max_pool))\n",
    "\n",
    "    attention = layers.Add()([avg_out, max_out])\n",
    "    attention = layers.Reshape((1, 1, channel))(attention)\n",
    "    return layers.Multiply()([input_feature, attention])\n",
    "\n",
    "\n",
    "def MACNN(input_shape, num_classes, l2_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Improved Multi-Attention CNN with:\n",
    "    - Better weight initialization (He normal)\n",
    "    - L2 regularization to prevent overfitting\n",
    "    - Optimized dropout rates\n",
    "    - Proper activation functions\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images (height, width, channels)\n",
    "        num_classes: Number of output classes\n",
    "        l2_reg: L2 regularization factor\n",
    "    \n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # First Convolutional Block\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Add Attention Module\n",
    "    x = attention_module(x)\n",
    "    x = spatial_attention(x)\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Add Attention Module\n",
    "    x = attention_module(x)\n",
    "    x = channel_attention(x)\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Flatten spatial dimensions before dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Dense Layers with proper regularization\n",
    "    x = layers.Dense(512, activation='relu',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                           kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "print(\"✓ Improved MACNN architecture loaded\")\n",
    "print(\"  - Double conv layers per block for better feature extraction\")\n",
    "print(\"  - He normal initialization for ReLU activations\")\n",
    "print(\"  - L2 regularization to prevent overfitting\")\n",
    "print(\"  - Optimized dropout rates\")\n",
    "print(\"  - Additional dense layer for better classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-09-02T08:50:07.758245Z",
     "iopub.status.busy": "2024-09-02T08:50:07.757913Z",
     "iopub.status.idle": "2024-09-02T08:52:19.907525Z",
     "shell.execute_reply": "2024-09-02T08:52:19.906480Z",
     "shell.execute_reply.started": "2024-09-02T08:50:07.758203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "DATA_ROOT = Path.cwd() / \"Datasets\" / \"RealAIGI\"   \n",
    "TARGET_SIZE = (256, 256)                                   \n",
    "COLOR_MODE = \"rgb\"                                       \n",
    "\n",
    "def load_local_dataset(data_root: Path,\n",
    "                       target_size=(256, 256),\n",
    "                       color_mode=\"rgb\"):\n",
    "    if not data_root.exists():\n",
    "        raise FileNotFoundError(f\"{data_root} does not exist\")\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for class_dir in sorted(data_root.iterdir()):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "        for image_path in class_dir.glob(\"*\"):\n",
    "            if image_path.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}:\n",
    "                continue  # skip unknown file types\n",
    "\n",
    "            img = tf.keras.utils.load_img(\n",
    "                image_path,\n",
    "                target_size=target_size,\n",
    "                color_mode=color_mode\n",
    "            )\n",
    "            arr = tf.keras.utils.img_to_array(img) / 255.0\n",
    "            images.append(arr)\n",
    "            labels.append(class_dir.name)\n",
    "\n",
    "    if not images:\n",
    "        raise ValueError(f\"No images found under {data_root}\")\n",
    "\n",
    "    image_array = np.stack(images, axis=0)\n",
    "    lb = LabelBinarizer()\n",
    "    label_array = lb.fit_transform(labels)\n",
    "    if label_array.ndim == 1:  # binary case\n",
    "        label_array = label_array[:, np.newaxis]\n",
    "\n",
    "    return image_array, label_array, lb.classes_\n",
    "\n",
    "image_list, label_list, class_names = load_local_dataset(\n",
    "    DATA_ROOT,\n",
    "    target_size=TARGET_SIZE,\n",
    "    color_mode=COLOR_MODE\n",
    ")\n",
    "num_classes = label_list.shape[1]\n",
    "input_shape = image_list.shape[1:]\n",
    "\n",
    "print(f\"Loaded custom dataset: {image_list.shape[0]} samples, image shape {input_shape}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.348800Z",
     "iopub.status.idle": "2024-09-02T08:52:20.349208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.350025Z",
     "iopub.status.idle": "2024-09-02T08:52:20.350452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.351254Z",
     "iopub.status.idle": "2024-09-02T08:52:20.351662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=100, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.352664Z",
     "iopub.status.idle": "2024-09-02T08:52:20.353071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# client_names = ['{}_{}'.format('client', i+1) for i in range(100)]\n",
    "# s = clients['client_1'][0][1]*0\n",
    "# for c in client_names:\n",
    "#     sum = clients[c][0][1]\n",
    "#     for i in range(1,378):\n",
    "#         sum = sum + clients[c][i][1]\n",
    "        \n",
    "#     s = s + sum/378\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.354194Z",
     "iopub.status.idle": "2024-09-02T08:52:20.354617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.355459Z",
     "iopub.status.idle": "2024-09-02T08:52:20.355849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED TRAINING HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Learning rate with scheduling\n",
    "initial_lr = 0.01\n",
    "comms_round = 100  # Increased from 50 to 100 for better convergence\n",
    "local_epochs = 3   # Multiple local epochs per round (was 1)\n",
    "batch_size = 32    # Batch size for training\n",
    "\n",
    "# Loss and metrics\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Optimizer with momentum and decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=comms_round * 10,  # Decay over rounds\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Client selection parameters\n",
    "num_clients_per_round = 10  # Number of clients to sample each round\n",
    "min_clients_per_round = 5   # Minimum clients needed\n",
    "\n",
    "print(f\"✓ Training hyperparameters configured:\")\n",
    "print(f\"  - Initial learning rate: {initial_lr}\")\n",
    "print(f\"  - Communication rounds: {comms_round}\")\n",
    "print(f\"  - Local epochs per round: {local_epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Clients per round: {num_clients_per_round}\")\n",
    "print(f\"  - Learning rate schedule: ExponentialDecay (decay_rate=0.96)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.357069Z",
     "iopub.status.idle": "2024-09-02T08:52:20.357536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#initialize global model\n",
    "global_model = MACNN(input_shape, num_classes)\n",
    "global_acc_list = []\n",
    "global_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.358533Z",
     "iopub.status.idle": "2024-09-02T08:52:20.358954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED FEDERATED LEARNING TRAINING LOOP (IID)\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize tracking lists\n",
    "global_acc_list = []\n",
    "global_loss_list = []\n",
    "divergence_list = []\n",
    "per_round_time = []\n",
    "\n",
    "# Reset cumulative time\n",
    "cumulative_time = 0.0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING IMPROVED FEDERATED LEARNING - IID SETTING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total clients: {len(clients_batched)}\")\n",
    "print(f\"Clients per round: {num_clients_per_round}\")\n",
    "print(f\"Communication rounds: {comms_round}\")\n",
    "print(f\"Local epochs: {local_epochs}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training loop\n",
    "for comm_round in range(comms_round):\n",
    "    round_start_time = time.time()\n",
    "    \n",
    "    # Get current global weights\n",
    "    global_weights = global_model.get_weights()\n",
    "    scaled_local_weight_list = []\n",
    "    \n",
    "    # Sample clients for this round\n",
    "    all_client_names = list(clients_batched.keys())\n",
    "    num_available_clients = len(all_client_names)\n",
    "    num_selected = min(num_clients_per_round, num_available_clients)\n",
    "    client_names = random.sample(all_client_names, k=num_selected)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nRound {comm_round}: Selected clients: {client_names[:5]}...\")\n",
    "    \n",
    "    # Local training on selected clients\n",
    "    for client in client_names:\n",
    "        # Create local model\n",
    "        local_model = MACNN(input_shape, num_classes, l2_reg=1e-4)\n",
    "        \n",
    "        # Create optimizer with current learning rate\n",
    "        current_lr = initial_lr * (0.96 ** (comm_round // 10))\n",
    "        local_optimizer = SGD(\n",
    "            learning_rate=current_lr,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        local_model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=local_optimizer,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        # Set weights from global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        # Train for multiple local epochs\n",
    "        local_model.fit(\n",
    "            clients_batched[client],\n",
    "            epochs=local_epochs,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Calculate scaling factor based on client data size\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        \n",
    "        # Scale model weights\n",
    "        scaled_weights = scale_model_weights(\n",
    "            local_model.get_weights(),\n",
    "            scaling_factor\n",
    "        )\n",
    "        \n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        # Clean up\n",
    "        del local_model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Aggregate weights with threshold normalization\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    # Compute weight divergence for monitoring\n",
    "    divergence = compute_weight_divergence(global_weights, average_weights)\n",
    "    divergence_list.append(divergence)\n",
    "    \n",
    "    # Update global model\n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    for (X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        global_acc_list.append(global_acc)\n",
    "        global_loss_list.append(global_loss)\n",
    "    \n",
    "    # Track round time\n",
    "    round_time = time.time() - round_start_time\n",
    "    per_round_time.append(round_time)\n",
    "    \n",
    "    # Print progress every 10 rounds\n",
    "    if (comm_round + 1) % 10 == 0:\n",
    "        avg_time = np.mean(per_round_time[-10:])\n",
    "        print(f\"\\n[Round {comm_round+1}/{comms_round}] \"\n",
    "              f\"Acc: {global_acc:.4f} | Loss: {global_loss:.4f} | \"\n",
    "              f\"Divergence: {divergence:.4f} | \"\n",
    "              f\"Avg Time: {avg_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEDERATED LEARNING COMPLETED - IID SETTING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Accuracy: {global_acc_list[-1]:.4f}\")\n",
    "print(f\"Final Loss: {global_loss_list[-1]:.4f}\")\n",
    "print(f\"Total Time: {cumulative_time:.2f} seconds\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.359896Z",
     "iopub.status.idle": "2024-09-02T08:52:20.360332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE VISUALIZATION AND ANALYSIS (IID)\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Accuracy over communication rounds\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "plt.plot(range(len(global_acc_list)), global_acc_list, 'b-', linewidth=2, label='Global Accuracy')\n",
    "plt.fill_between(range(len(global_acc_list)), global_acc_list, alpha=0.3)\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Global Model Accuracy (IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 2. Loss over communication rounds\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "plt.plot(range(len(global_loss_list)), global_loss_list, 'r-', linewidth=2, label='Global Loss')\n",
    "plt.fill_between(range(len(global_loss_list)), global_loss_list, alpha=0.3, color='red')\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Global Model Loss (IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Weight Divergence\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "plt.plot(range(len(divergence_list)), divergence_list, 'g-', linewidth=2, label='Weight Divergence')\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('L2 Divergence', fontsize=12)\n",
    "plt.title('Model Update Divergence (IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Moving average accuracy (window=10)\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "window_size = 10\n",
    "if len(global_acc_list) >= window_size:\n",
    "    moving_avg = np.convolve(global_acc_list, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(len(moving_avg)), moving_avg, 'purple', linewidth=2, label=f'MA({window_size})')\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Accuracy (MA)', fontsize=12)\n",
    "    plt.title(f'Moving Average Accuracy (IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 5. Accuracy improvement rate\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "if len(global_acc_list) > 1:\n",
    "    acc_diff = np.diff(global_acc_list)\n",
    "    plt.plot(range(len(acc_diff)), acc_diff, 'orange', linewidth=2, label='Δ Accuracy')\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Accuracy Change', fontsize=12)\n",
    "    plt.title('Accuracy Improvement Rate (IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 6. Loss improvement rate\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "if len(global_loss_list) > 1:\n",
    "    loss_diff = np.diff(global_loss_list)\n",
    "    plt.plot(range(len(loss_diff)), loss_diff, 'brown', linewidth=2, label='Δ Loss')\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Loss Change', fontsize=12)\n",
    "    plt.title('Loss Improvement Rate (IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 7. Cumulative time per round\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "if len(per_round_time) > 0:\n",
    "    plt.plot(range(len(per_round_time)), np.cumsum(per_round_time), 'cyan', linewidth=2, label='Cumulative Time')\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.title('Cumulative Training Time (IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 8. Statistics summary table\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "ax8.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "TRAINING STATISTICS (IID)\n",
    "\n",
    "Total Rounds: {len(global_acc_list)}\n",
    "─────────────────────────\n",
    "Accuracy:\n",
    "  Initial:  {global_acc_list[0]:.4f}\n",
    "  Final:    {global_acc_list[-1]:.4f}\n",
    "  Max:      {max(global_acc_list):.4f}\n",
    "  Mean:     {np.mean(global_acc_list):.4f}\n",
    "  Std:      {np.std(global_acc_list):.4f}\n",
    "─────────────────────────\n",
    "Loss:\n",
    "  Initial:  {global_loss_list[0]:.4f}\n",
    "  Final:    {global_loss_list[-1]:.4f}\n",
    "  Min:      {min(global_loss_list):.4f}\n",
    "  Mean:     {np.mean(global_loss_list):.4f}\n",
    "  Std:      {np.std(global_loss_list):.4f}\n",
    "─────────────────────────\n",
    "Training Time:\n",
    "  Total:    {cumulative_time:.2f}s\n",
    "  Per Round:{cumulative_time/len(global_acc_list):.2f}s\n",
    "─────────────────────────\n",
    "Model Stability:\n",
    "  Avg Divergence: {np.mean(divergence_list):.4f}\n",
    "  Max Divergence: {max(divergence_list):.4f}\n",
    "\"\"\"\n",
    "ax8.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "# 9. Accuracy distribution\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "plt.hist(global_acc_list, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.axvline(np.mean(global_acc_list), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "plt.axvline(np.median(global_acc_list), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "plt.xlabel('Accuracy', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Accuracy Distribution (IID)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fl_iid_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IID SETTING - COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Visualization saved as 'fl_iid_comprehensive_analysis.png'\")\n",
    "print(f\"✓ Total communication rounds: {len(global_acc_list)}\")\n",
    "print(f\"✓ Final accuracy: {global_acc_list[-1]:.4f} (improvement: {global_acc_list[-1]-global_acc_list[0]:.4f})\")\n",
    "print(f\"✓ Final loss: {global_loss_list[-1]:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.361177Z",
     "iopub.status.idle": "2024-09-02T08:52:20.361572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "iid_df = pd.DataFrame(list(zip(global_acc_list, global_loss_list)), columns =['global_acc_list', 'global_loss_list'])\n",
    "iid_df.to_csv('MNIST_IID.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.362558Z",
     "iopub.status.idle": "2024-09-02T08:52:20.363224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=100, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    # data = list(zip(image_list, label_list))\n",
    "    # random.shuffle(data)  # <- IID\n",
    "    \n",
    "    # sort data for non-iid\n",
    "    max_y = np.argmax(label_list, axis=-1)\n",
    "    sorted_zip = sorted(zip(max_y, label_list, image_list), key=lambda x: x[0])\n",
    "    data = [(x,y) for _,y,x in sorted_zip]\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.364018Z",
     "iopub.status.idle": "2024-09-02T08:52:20.364426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.365221Z",
     "iopub.status.idle": "2024-09-02T08:52:20.365618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=100, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.366517Z",
     "iopub.status.idle": "2024-09-02T08:52:20.366908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.367779Z",
     "iopub.status.idle": "2024-09-02T08:52:20.368224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS FOR NON-IID\n",
    "# ============================================================================\n",
    "\n",
    "# Toggle to quickly sanity-check the training loop without long runtimes\n",
    "quick_debug_mode = True  # Set to False for full-length training\n",
    "\n",
    "# Base hyperparameters (full training defaults)\n",
    "initial_lr_noniid = 0.008  # Slightly lower LR for non-IID stability\n",
    "comms_round_noniid = 150   # More rounds needed for non-IID convergence\n",
    "local_epochs_noniid = 3\n",
    "batch_size_noniid = 32\n",
    "num_clients_per_round_noniid = 10\n",
    "\n",
    "# Runtime safety controls (None disables the safeguard)\n",
    "max_batches_per_client_noniid = None  # Limits batches per client per round\n",
    "max_total_minutes_noniid = None       # Stops training after X minutes\n",
    "target_accuracy_noniid = None         # Stops when accuracy threshold reached\n",
    "\n",
    "if quick_debug_mode:\n",
    "    # Shrink the workload for quick experimentation / debugging sessions\n",
    "    comms_round_noniid = min(comms_round_noniid, 20)\n",
    "    local_epochs_noniid = min(local_epochs_noniid, 2)\n",
    "    num_clients_per_round_noniid = min(num_clients_per_round_noniid, 5)\n",
    "    max_batches_per_client_noniid = 5\n",
    "    max_total_minutes_noniid = 5  # minutes\n",
    "    target_accuracy_noniid = 0.65\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "print(\"✓ Non-IID Training hyperparameters configured:\")\n",
    "print(f\"  - Quick debug mode: {'ENABLED' if quick_debug_mode else 'disabled'}\")\n",
    "print(f\"  - Initial learning rate: {initial_lr_noniid}\")\n",
    "print(f\"  - Communication rounds (max): {comms_round_noniid}\")\n",
    "print(f\"  - Local epochs per round: {local_epochs_noniid}\")\n",
    "print(f\"  - Batch size: {batch_size_noniid}\")\n",
    "print(f\"  - Clients per round: {num_clients_per_round_noniid}\")\n",
    "print(f\"  - Max batches per client: {max_batches_per_client_noniid if max_batches_per_client_noniid is not None else 'unlimited'}\")\n",
    "print(f\"  - Time budget (minutes): {max_total_minutes_noniid if max_total_minutes_noniid is not None else 'unlimited'}\")\n",
    "print(f\"  - Early-stop accuracy: {target_accuracy_noniid if target_accuracy_noniid is not None else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.369162Z",
     "iopub.status.idle": "2024-09-02T08:52:20.369599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#initialize global model\n",
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10\n",
    "global_model = MACNN(input_shape, num_classes)\n",
    "global_acc_list = []\n",
    "global_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.370496Z",
     "iopub.status.idle": "2024-09-02T08:52:20.370907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED FEDERATED LEARNING TRAINING LOOP (NON-IID)\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize tracking lists for Non-IID\n",
    "global_acc_list_noniid = []\n",
    "global_loss_list_noniid = []\n",
    "divergence_list_noniid = []\n",
    "per_round_time_noniid = []\n",
    "\n",
    "# Reset cumulative time\n",
    "cumulative_time = 0.0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING IMPROVED FEDERATED LEARNING - NON-IID SETTING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total clients: {len(clients_batched)}\")\n",
    "print(f\"Clients per round: {num_clients_per_round_noniid}\")\n",
    "print(f\"Communication rounds (max): {comms_round_noniid}\")\n",
    "print(f\"Local epochs: {local_epochs_noniid}\")\n",
    "print(f\"⚠️  NON-IID: Clients have non-uniformly distributed data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "overall_start_time = time.time()\n",
    "stop_reason = None\n",
    "interrupted = False\n",
    "\n",
    "try:\n",
    "    for comm_round in range(comms_round_noniid):\n",
    "        round_start_time = time.time()\n",
    "\n",
    "        # Get current global weights\n",
    "        global_weights = global_model.get_weights()\n",
    "        scaled_local_weight_list = []\n",
    "\n",
    "        # Sample clients for this round\n",
    "        all_client_names = list(clients_batched.keys())\n",
    "        num_available_clients = len(all_client_names)\n",
    "        num_selected = min(num_clients_per_round_noniid, num_available_clients)\n",
    "\n",
    "        if num_selected == 0:\n",
    "            stop_reason = \"No clients available for selection\"\n",
    "            break\n",
    "\n",
    "        client_names = random.sample(all_client_names, k=num_selected)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nRound {comm_round}: Selected clients: {client_names[:5]}...\")\n",
    "\n",
    "        # Determine effective sample sizes for selected clients\n",
    "        if max_batches_per_client_noniid is not None:\n",
    "            client_effective_sizes = {\n",
    "                name: min(len(clients[name]), max_batches_per_client_noniid * batch_size_noniid)\n",
    "                for name in client_names\n",
    "            }\n",
    "        else:\n",
    "            client_effective_sizes = {name: len(clients[name]) for name in client_names}\n",
    "\n",
    "        round_total_samples = max(sum(client_effective_sizes.values()), 1)\n",
    "\n",
    "        # Local training on selected clients\n",
    "        for client in client_names:\n",
    "            local_model = MACNN(input_shape, num_classes, l2_reg=1e-4)\n",
    "\n",
    "            # Create optimizer with current learning rate (slow decay for non-IID)\n",
    "            current_lr = initial_lr_noniid * (0.98 ** (comm_round // 15))\n",
    "            local_optimizer = SGD(\n",
    "                learning_rate=current_lr,\n",
    "                momentum=0.9,\n",
    "                nesterov=True\n",
    "            )\n",
    "\n",
    "            local_model.compile(\n",
    "                loss=loss,\n",
    "                optimizer=local_optimizer,\n",
    "                metrics=metrics\n",
    "            )\n",
    "\n",
    "            # Set weights from global model\n",
    "            local_model.set_weights(global_weights)\n",
    "\n",
    "            # Prepare dataset (optionally capped for faster iterations)\n",
    "            client_dataset = clients_batched[client]\n",
    "            if max_batches_per_client_noniid is not None:\n",
    "                client_dataset = client_dataset.take(max_batches_per_client_noniid)\n",
    "            client_dataset = client_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Train for multiple local epochs\n",
    "            local_model.fit(\n",
    "                client_dataset,\n",
    "                epochs=local_epochs_noniid,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Calculate scaling factor based on effective client data size\n",
    "            scaling_factor = client_effective_sizes[client] / round_total_samples\n",
    "\n",
    "            # Scale model weights and collect\n",
    "            scaled_weights = scale_model_weights(\n",
    "                local_model.get_weights(),\n",
    "                scaling_factor\n",
    "            )\n",
    "            scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "            # Clean up\n",
    "            del local_model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        if not scaled_local_weight_list:\n",
    "            stop_reason = \"No client updates were gathered in this round\"\n",
    "            break\n",
    "\n",
    "        # Aggregate weights with threshold normalization\n",
    "        average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "        # Compute weight divergence for monitoring\n",
    "        divergence = compute_weight_divergence(global_weights, average_weights)\n",
    "        divergence_list_noniid.append(divergence)\n",
    "\n",
    "        # Update global model\n",
    "        global_model.set_weights(average_weights)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        global_acc = None\n",
    "        global_loss = None\n",
    "        for (X_test, Y_test) in test_batched:\n",
    "            global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "            global_acc_list_noniid.append(global_acc)\n",
    "            global_loss_list_noniid.append(global_loss)\n",
    "\n",
    "        # Track round time\n",
    "        round_time = time.time() - round_start_time\n",
    "        per_round_time_noniid.append(round_time)\n",
    "\n",
    "        # Frequent progress updates for long runs\n",
    "        if (comm_round + 1) % 5 == 0 or comm_round == 0:\n",
    "            display_acc = f\"{global_acc:.4f}\" if global_acc is not None else \"N/A\"\n",
    "            display_loss = f\"{global_loss:.4f}\" if global_loss is not None else \"N/A\"\n",
    "            display_div = f\"{divergence:.4f}\"\n",
    "            print(\n",
    "                f\"[Round {comm_round+1}/{comms_round_noniid}] \"\n",
    "                f\"Acc: {display_acc} | Loss: {display_loss} | \"\n",
    "                f\"Divergence: {display_div} | Time: {round_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "        # Early stopping conditions\n",
    "        if target_accuracy_noniid is not None and global_acc is not None and global_acc >= target_accuracy_noniid:\n",
    "            stop_reason = (\n",
    "                f\"Target accuracy reached ({global_acc:.4f} ≥ {target_accuracy_noniid:.4f})\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        elapsed_minutes = (time.time() - overall_start_time) / 60.0\n",
    "        if max_total_minutes_noniid is not None and elapsed_minutes >= max_total_minutes_noniid:\n",
    "            stop_reason = (\n",
    "                f\"Time budget exceeded ({elapsed_minutes:.1f} min ≥ {max_total_minutes_noniid} min)\"\n",
    "            )\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    interrupted = True\n",
    "    stop_reason = \"Manual interruption detected (KeyboardInterrupt)\"\n",
    "finally:\n",
    "    total_elapsed_minutes = (time.time() - overall_start_time) / 60.0\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEDERATED LEARNING SUMMARY - NON-IID SETTING\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if interrupted:\n",
    "        print(\"⚠️  Training interrupted by user; partial progress preserved.\")\n",
    "    elif stop_reason is not None:\n",
    "        print(f\"⚠️  Training stopped early: {stop_reason}\")\n",
    "    else:\n",
    "        print(\"✅ Training completed all scheduled rounds.\")\n",
    "\n",
    "    rounds_completed = len(global_acc_list_noniid)\n",
    "    print(f\"Rounds completed: {rounds_completed}\")\n",
    "    print(f\"Total elapsed time: {total_elapsed_minutes:.2f} minutes\")\n",
    "\n",
    "    if global_acc_list_noniid:\n",
    "        print(f\"Final Accuracy: {global_acc_list_noniid[-1]:.4f}\")\n",
    "        print(f\"Final Loss: {global_loss_list_noniid[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"No evaluation metrics were recorded before stopping.\")\n",
    "\n",
    "    print(f\"Total time tracked via test_model: {cumulative_time:.2f} seconds\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.371875Z",
     "iopub.status.idle": "2024-09-02T08:52:20.372335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE VISUALIZATION AND ANALYSIS (NON-IID)\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Accuracy over communication rounds\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "plt.plot(range(len(global_acc_list_noniid)), global_acc_list_noniid, 'b-', linewidth=2, label='Global Accuracy')\n",
    "plt.fill_between(range(len(global_acc_list_noniid)), global_acc_list_noniid, alpha=0.3)\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Global Model Accuracy (Non-IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 2. Loss over communication rounds\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "plt.plot(range(len(global_loss_list_noniid)), global_loss_list_noniid, 'r-', linewidth=2, label='Global Loss')\n",
    "plt.fill_between(range(len(global_loss_list_noniid)), global_loss_list_noniid, alpha=0.3, color='red')\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Global Model Loss (Non-IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Weight Divergence\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "plt.plot(range(len(divergence_list_noniid)), divergence_list_noniid, 'g-', linewidth=2, label='Weight Divergence')\n",
    "plt.xlabel('Communication Round', fontsize=12)\n",
    "plt.ylabel('L2 Divergence', fontsize=12)\n",
    "plt.title('Model Update Divergence (Non-IID)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Moving average accuracy (window=10)\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "window_size = 10\n",
    "if len(global_acc_list_noniid) >= window_size:\n",
    "    moving_avg = np.convolve(global_acc_list_noniid, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(len(moving_avg)), moving_avg, 'purple', linewidth=2, label=f'MA({window_size})')\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Accuracy (MA)', fontsize=12)\n",
    "    plt.title(f'Moving Average Accuracy (Non-IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 5. Accuracy improvement rate\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "if len(global_acc_list_noniid) > 1:\n",
    "    acc_diff = np.diff(global_acc_list_noniid)\n",
    "    plt.plot(range(len(acc_diff)), acc_diff, 'orange', linewidth=2, label='Δ Accuracy')\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Accuracy Change', fontsize=12)\n",
    "    plt.title('Accuracy Improvement Rate (Non-IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 6. Loss improvement rate\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "if len(global_loss_list_noniid) > 1:\n",
    "    loss_diff = np.diff(global_loss_list_noniid)\n",
    "    plt.plot(range(len(loss_diff)), loss_diff, 'brown', linewidth=2, label='Δ Loss')\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Loss Change', fontsize=12)\n",
    "    plt.title('Loss Improvement Rate (Non-IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 7. Cumulative time per round\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "if len(per_round_time_noniid) > 0:\n",
    "    plt.plot(range(len(per_round_time_noniid)), np.cumsum(per_round_time_noniid), 'cyan', linewidth=2, label='Cumulative Time')\n",
    "    plt.xlabel('Communication Round', fontsize=12)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.title('Cumulative Training Time (Non-IID)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "# 8. Statistics summary table\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "ax8.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "TRAINING STATISTICS (NON-IID)\n",
    "\n",
    "Total Rounds: {len(global_acc_list_noniid)}\n",
    "─────────────────────────\n",
    "Accuracy:\n",
    "  Initial:  {global_acc_list_noniid[0]:.4f}\n",
    "  Final:    {global_acc_list_noniid[-1]:.4f}\n",
    "  Max:      {max(global_acc_list_noniid):.4f}\n",
    "  Mean:     {np.mean(global_acc_list_noniid):.4f}\n",
    "  Std:      {np.std(global_acc_list_noniid):.4f}\n",
    "─────────────────────────\n",
    "Loss:\n",
    "  Initial:  {global_loss_list_noniid[0]:.4f}\n",
    "  Final:    {global_loss_list_noniid[-1]:.4f}\n",
    "  Min:      {min(global_loss_list_noniid):.4f}\n",
    "  Mean:     {np.mean(global_loss_list_noniid):.4f}\n",
    "  Std:      {np.std(global_loss_list_noniid):.4f}\n",
    "─────────────────────────\n",
    "Training Time:\n",
    "  Total:    {cumulative_time:.2f}s\n",
    "  Per Round:{cumulative_time/len(global_acc_list_noniid):.2f}s\n",
    "─────────────────────────\n",
    "Model Stability:\n",
    "  Avg Divergence: {np.mean(divergence_list_noniid):.4f}\n",
    "  Max Divergence: {max(divergence_list_noniid):.4f}\n",
    "\"\"\"\n",
    "ax8.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "# 9. Accuracy distribution\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "plt.hist(global_acc_list_noniid, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "plt.axvline(np.mean(global_acc_list_noniid), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "plt.axvline(np.median(global_acc_list_noniid), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "plt.xlabel('Accuracy', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Accuracy Distribution (Non-IID)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fl_noniid_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NON-IID SETTING - COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Visualization saved as 'fl_noniid_comprehensive_analysis.png'\")\n",
    "print(f\"✓ Total communication rounds: {len(global_acc_list_noniid)}\")\n",
    "print(f\"✓ Final accuracy: {global_acc_list_noniid[-1]:.4f} (improvement: {global_acc_list_noniid[-1]-global_acc_list_noniid[0]:.4f})\")\n",
    "print(f\"✓ Final loss: {global_loss_list_noniid[-1]:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T08:52:20.373597Z",
     "iopub.status.idle": "2024-09-02T08:52:20.374005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "noniid_df = pd.DataFrame(list(zip(global_acc_list, global_loss_list)), columns =['global_acc_list', 'global_loss_list'])\n",
    "noniid_df.to_csv('CIFAR-10_Non-IID.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARISON: IID vs NON-IID\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "axes[0, 0].plot(range(len(global_acc_list)), global_acc_list, 'b-', linewidth=2, label='IID', alpha=0.8)\n",
    "axes[0, 0].plot(range(len(global_acc_list_noniid)), global_acc_list_noniid, 'r-', linewidth=2, label='Non-IID', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Communication Round', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Accuracy: IID vs Non-IID', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss Comparison\n",
    "axes[0, 1].plot(range(len(global_loss_list)), global_loss_list, 'b-', linewidth=2, label='IID', alpha=0.8)\n",
    "axes[0, 1].plot(range(len(global_loss_list_noniid)), global_loss_list_noniid, 'r-', linewidth=2, label='Non-IID', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Communication Round', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "axes[0, 1].set_title('Loss: IID vs Non-IID', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Divergence Comparison\n",
    "axes[0, 2].plot(range(len(divergence_list)), divergence_list, 'b-', linewidth=2, label='IID', alpha=0.8)\n",
    "axes[0, 2].plot(range(len(divergence_list_noniid)), divergence_list_noniid, 'r-', linewidth=2, label='Non-IID', alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Communication Round', fontsize=11)\n",
    "axes[0, 2].set_ylabel('Weight Divergence', fontsize=11)\n",
    "axes[0, 2].set_title('Model Stability: IID vs Non-IID', fontsize=13, fontweight='bold')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Accuracy Distribution Comparison\n",
    "axes[1, 0].hist(global_acc_list, bins=20, alpha=0.6, label='IID', edgecolor='black', color='blue')\n",
    "axes[1, 0].hist(global_acc_list_noniid, bins=20, alpha=0.6, label='Non-IID', edgecolor='black', color='red')\n",
    "axes[1, 0].set_xlabel('Accuracy', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Accuracy Distribution Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Box Plot Comparison\n",
    "axes[1, 1].boxplot([global_acc_list, global_acc_list_noniid], \n",
    "                    labels=['IID', 'Non-IID'],\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1, 1].set_title('Accuracy Box Plot Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Summary Statistics Table\n",
    "axes[1, 2].axis('off')\n",
    "comparison_text = f\"\"\"\n",
    "COMPARATIVE ANALYSIS\n",
    "════════════════════════════════════\n",
    "\n",
    "IID Setting:\n",
    "  Final Acc:     {global_acc_list[-1]:.4f}\n",
    "  Max Acc:       {max(global_acc_list):.4f}\n",
    "  Mean Acc:      {np.mean(global_acc_list):.4f}\n",
    "  Std Acc:       {np.std(global_acc_list):.4f}\n",
    "  Final Loss:    {global_loss_list[-1]:.4f}\n",
    "  \n",
    "Non-IID Setting:\n",
    "  Final Acc:     {global_acc_list_noniid[-1]:.4f}\n",
    "  Max Acc:       {max(global_acc_list_noniid):.4f}\n",
    "  Mean Acc:      {np.mean(global_acc_list_noniid):.4f}\n",
    "  Std Acc:       {np.std(global_acc_list_noniid):.4f}\n",
    "  Final Loss:    {global_loss_list_noniid[-1]:.4f}\n",
    "\n",
    "Performance Gap:\n",
    "  Acc Difference: {abs(global_acc_list[-1] - global_acc_list_noniid[-1]):.4f}\n",
    "  Loss Difference: {abs(global_loss_list[-1] - global_loss_list_noniid[-1]):.4f}\n",
    "\n",
    "Convergence:\n",
    "  IID Rounds:    {len(global_acc_list)}\n",
    "  Non-IID Rounds: {len(global_acc_list_noniid)}\n",
    "\"\"\"\n",
    "axes[1, 2].text(0.1, 0.5, comparison_text, fontsize=10, family='monospace',\n",
    "                verticalalignment='center', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fl_iid_vs_noniid_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE COMPARISON: IID vs NON-IID\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Comparison visualization saved as 'fl_iid_vs_noniid_comparison.png'\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 46718,
     "sourceId": 3649,
     "sourceType": "competition"
    },
    {
     "datasetId": 1272,
     "sourceId": 2280,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 118250,
     "sourceId": 283795,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30097,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cooolenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
